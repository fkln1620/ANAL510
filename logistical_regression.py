# -*- coding: utf-8 -*-
"""Logistical Regression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gfu5QUgl3ULzzknRS5yF5ROGxxjmRWQk
"""

import numpy as np
import pandas as pd
data = pd.read_csv('/content/drive/MyDrive/data/agaricus-lepiota.csv')
data = data.replace({'?': np.nan}).dropna(axis=1)

"""Attribute Information: (classes: edible=e, poisonous=p)
  1. cap-shape:                bell=b,conical=c,convex=x,flat=f,knobbed=k,sunken=s
  2. cap-surface:              fibrous=f,grooves=g,scaly=y,smooth=s
  3. cap-color:                brown=n,buff=b,cinnamon=c,gray=g,green=r,pink=p,purple=u,red=e,white=w,yellow=y
  4. bruises?:                 bruises=t,no=f
  5. odor:                     almond=a,anise=l,creosote=c,fishy=y,foul=f,
musty=m,none=n,pungent=p,spicy=s
  6. gill-attachment:          attached=a,descending=d,free=f,notched=n
  7. gill-spacing:             close=c,crowded=w,distant=d
  8. gill-size:                broad=b,narrow=n
  9. gill-color:               black=k,brown=n,buff=b,chocolate=h,gray=g,green=r,orange=o,pink=p,purple=u,red=e,white=w,yellow=y
  10. stalk-shape:              enlarging=e,tapering=t
  11. stalk-root:               bulbous=b,club=c,cup=u,equal=e,rhizomorphs=z,rooted=r,missing=?
  12. stalk-surface-above-ring: fibrous=f,scaly=y,silky=k,smooth=s
  13. stalk-surface-below-ring: fibrous=f,scaly=y,silky=k,smooth=s
  14. stalk-color-above-ring:   brown=n,buff=b,cinnamon=c,gray=g,orange=o,pink=p,red=e,white=w,yellow=y
  15. stalk-color-below-ring:   brown=n,buff=b,cinnamon=c,gray=g,orange=o,pink=p,red=e,white=w,yellow=y
  16. veil-type:                partial=p,universal=u
  17. veil-color:               brown=n,orange=o,white=w,yellow=y
  18. ring-number:              none=n,one=o,two=t
  19. ring-type:                cobwebby=c,evanescent=e,flaring=f,large=l,none=n,pendant=p,sheathing=s,zone=z
  20. spore-print-color:        black=k,brown=n,buff=b,chocolate=h,green=r,orange=o,purple=u,white=w,yellow=y
  21. population:               abundant=a,clustered=c,numerous=n,scattered=s,several=v,solitary=y
  22. habitat:                  grasses=g,leaves=l,meadows=m,paths=p,urban=u,waste=w,woods=d

  We are dropping stalk-root for its missing values.
"""

data

for col in data.columns:
  display(data[col].value_counts())

"""veil_type has only one level so it essentially doesn't provide any useful information here. Thus we remove this variable."""

pd.crosstab(data['stalk_color_above_ring'], data['stalk_color_below_ring'])

"""Notice that the diagonal values are large, meaning the two variables are very similar in distribution, providing similar information and resulting in multicollinearity. Consider removing one of them. Here we removed stalk_color_above_ring."""

import pandas as pd
from sklearn.model_selection import train_test_split

pd.set_option('future.no_silent_downcasting', True)
data_clean = data.drop(columns=['veil_type', 'stalk_color_above_ring'])
X = data_clean.drop(columns=['class'])
y = data_clean['class']
X_encoded = pd.get_dummies(X, drop_first=True)
X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=324762)

from sklearn.linear_model import LogisticRegression

model = LogisticRegression()
model.fit(X_train, y_train)

"""Attribute Information: (classes: edible=e, poisonous=p)
  1. cap-shape:                bell=b,conical=c,convex=x,flat=f,knobbed=k,sunken=s
  2. cap-surface:              fibrous=f,grooves=g,scaly=y,smooth=s
  3. cap-color:                brown=n,buff=b,cinnamon=c,gray=g,green=r,pink=p,purple=u,red=e,white=w,yellow=y
  4. bruises?:                 bruises=t,no=f
  5. odor:                     almond=a,anise=l,creosote=c,fishy=y,foul=f,
musty=m,none=n,pungent=p,spicy=s
  6. gill-attachment:          attached=a,descending=d,free=f,notched=n
  7. gill-spacing:             close=c,crowded=w,distant=d
  8. gill-size:                broad=b,narrow=n
  9. gill-color:               black=k,brown=n,buff=b,chocolate=h,gray=g,green=r,orange=o,pink=p,purple=u,red=e,white=w,yellow=y
  10. stalk-shape:              enlarging=e,tapering=t
  11. stalk-root:               bulbous=b,club=c,cup=u,equal=e,rhizomorphs=z,rooted=r,missing=?
  12. stalk-surface-above-ring: fibrous=f,scaly=y,silky=k,smooth=s
  13. stalk-surface-below-ring: fibrous=f,scaly=y,silky=k,smooth=s
  14. stalk-color-above-ring:   brown=n,buff=b,cinnamon=c,gray=g,orange=o,pink=p,red=e,white=w,yellow=y
  15. stalk-color-below-ring:   brown=n,buff=b,cinnamon=c,gray=g,orange=o,pink=p,red=e,white=w,yellow=y
  16. veil-type:                partial=p,universal=u
  17. veil-color:               brown=n,orange=o,white=w,yellow=y
  18. ring-number:              none=n,one=o,two=t
  19. ring-type:                cobwebby=c,evanescent=e,flaring=f,large=l,none=n,pendant=p,sheathing=s,zone=z
  20. spore-print-color:        black=k,brown=n,buff=b,chocolate=h,green=r,orange=o,purple=u,white=w,yellow=y
  21. population:               abundant=a,clustered=c,numerous=n,scattered=s,several=v,solitary=y
  22. habitat:                  grasses=g,leaves=l,meadows=m,paths=p,urban=u,waste=w,woods=d
"""

coefficients = pd.DataFrame({'Feature': X_train.columns, 'Coefficient': model.coef_[0]})
pd.set_option('display.max_rows', None)
display(coefficients)

"""The coefficients are in log odds. One example of how to interpret the coefficients: cap_shape_c has a coefficient estimate of 0.706. This means that all else held constant, the log odds of a type of mushroom being poisonous is 0.706 higher for the ones with conical shape than the ones with bell shape."""

predictions = model.predict(X_test)

from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Calculate Confusion Matrix
cm = confusion_matrix(y_test, predictions)

# Display Confusion Matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Predicted e', 'Predicted p'], yticklabels=['Actual e', 'Actual p'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

from sklearn.metrics import classification_report

# Print classification report
print(classification_report(y_test, predictions))

from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt

# Get predicted probabilities for the positive class ('p')
predictions_proba = model.predict_proba(X_test)[:, 1]

# Calculate ROC curve
fpr, tpr, thresholds = roc_curve(y_test, predictions_proba, pos_label='p')

# Calculate AUC
roc_auc = auc(fpr, tpr)

# Plot ROC curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

"""The confusion matrix, classification report, and the AUC-ROC curve shows that the model makes a perfect prediction on the test data. It predicts every data point with correct result and the AUC is 1, meaning the model is perfect in predicting the test data. This is rarely the case in real life but the results here shoes the power of logistic regression."""